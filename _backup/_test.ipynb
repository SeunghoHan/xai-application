{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3e76ad-a4d1-4362-9e08-1a2eeec06682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "# from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from data.data_loader import PowerConsumptionDataset, PowerWeatherDataset\n",
    "from models.lstm import LSTMModel\n",
    "from models.lstm_attention import LSTMWithAttention, BiLSTMWithAttention\n",
    "from models.gru import GRUModel\n",
    "from models.utils import create_model, train_and_evaluate, load_model, evaluate_r2_score\n",
    "from explainers.lime import LimeExplainer\n",
    "from explainers.shap import ShapExplainer\n",
    "from explainers.attention import AttentionExplainer\n",
    "from explainers.grad_cam import GradCAMExplainer\n",
    "from explainers.lrp import LRPExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88b5090-68d2-4d1e-a59d-1bf6280805fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bd7d046-fcc0-4ecd-8edb-28716cff5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LSTM' or 'GRU' for LIME and SHAP, 'LSTM_Attention' for 'Attention' \n",
    "model_name = 'LSTM'  \n",
    "model_name_attention = 'LSTM_Attention'  \n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "sequence_length = 60  # e.g., past 60 minutes\n",
    "feature_idx = [0,1,2,3,4,5,6]\n",
    "# feature_idx = []\n",
    "if len(feature_idx) == 0: num_of_features = 7\n",
    "else: num_of_features = len(feature_idx)\n",
    "input_size = num_of_features  # Number of total features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e40be1e-2502-4e1f-9161-62f8a58863d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d513040f-8f49-421e-b31c-10859e070b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerConsumptionDataset:\n",
    "    def __init__(self, file_path, sequence_length=24*30, prediction_length=24, target_features=[]):\n",
    "        self.file_path = file_path\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "        all_features = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "        \n",
    "        \n",
    "        if len(target_features) == 0:\n",
    "            # self.selected_features = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "            self.selected_features = ['Global_active_power', 'Global_intensity', \n",
    "                                      'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', \n",
    "                                      'Temp_Avg', 'Humidity_Avg', 'sin_hour', 'cos_hour', \n",
    "                                      'sin_day', 'cos_day', 'sin_month', 'cos_month']\n",
    "        else:\n",
    "            self.selected_features = target_features\n",
    "    \n",
    "        # self.selected_features = all_features[0:num_of_features]\n",
    "        \n",
    "    def load_data(self):\n",
    "        # Load and preprocess data\n",
    "        data = pd.read_csv(self.file_path)\n",
    "        data.drop(columns=['datetime'], errors='ignore', inplace=True)\n",
    "        # Fill missing values\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "        # Select all features\n",
    "        data_selected = data[self.selected_features]\n",
    "\n",
    "        # Normalize the data\n",
    "        data_scaled = self.scaler.fit_transform(data_selected.values)\n",
    "\n",
    "        # Create sequences\n",
    "        sequences, targets = self.create_sequences(data_scaled)\n",
    "\n",
    "        # Split the data into train and eval sets\n",
    "        train_sequences, eval_sequences, train_targets, eval_targets = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "        return train_sequences, eval_sequences, train_targets, eval_targets\n",
    "\n",
    "    # def create_sequences(self, data):\n",
    "    #     sequences = []\n",
    "    #     targets = []\n",
    "    #     for i in range(len(data) - self.sequence_length):\n",
    "    #         sequences.append(data[i:i + self.sequence_length])\n",
    "    #         targets.append(data[i + self.sequence_length, 0])  # Predicting 'Global_active_power'\n",
    "    #     return np.array(sequences), np.array(targets)\n",
    "    def create_sequences(self, data):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(data) - self.sequence_length - self.prediction_length):\n",
    "            sequences.append(data[i:i + self.sequence_length])\n",
    "            # Next 'prediction_length' values for 'Global_active_power' (first column)\n",
    "            targets.append(data[i + self.sequence_length : i + self.sequence_length + self.prediction_length, 0])\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "\n",
    "\n",
    "class PowerWeatherDataset:\n",
    "    def __init__(self, file_path, sequence_length=24*30, prediction_length=24, target_features=[]):\n",
    "        self.file_path = file_path\n",
    "        self.sequence_length = sequence_length  \n",
    "        self.prediction_length = prediction_length  \n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "        if len(target_features) == 0:\n",
    "            self.selected_features = ['Global_active_power', 'Global_intensity', \n",
    "                                      'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', \n",
    "                                      'Temp_Avg', 'Humidity_Avg', 'sin_hour', 'cos_hour', \n",
    "                                      'sin_day', 'cos_day', 'sin_month', 'cos_month']\n",
    "        else:\n",
    "            self.selected_features = target_features\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load and preprocess data\n",
    "        data = pd.read_csv(self.file_path)\n",
    "        data.drop(columns=['datetime'], errors='ignore', inplace=True)\n",
    "\n",
    "        \n",
    "        data.fillna(method='ffill', inplace=True)  # Use forward fill or interpolation for time series data\n",
    "        data.interpolate(method='linear', inplace=True)\n",
    "\n",
    "        # Scale selected features\n",
    "        data_scaled = self.scaler.fit_transform(data[self.selected_features].values)\n",
    "\n",
    "        # Train-test split before creating sequences\n",
    "        train_size = int(len(data_scaled) * 0.8)\n",
    "        train_data, eval_data = data_scaled[:train_size], data_scaled[train_size:]\n",
    "\n",
    "        # Create sequences for both train and eval\n",
    "        train_sequences, train_targets = self.create_sequences(train_data)\n",
    "        eval_sequences, eval_targets = self.create_sequences(eval_data)\n",
    "\n",
    "        return train_sequences, eval_sequences, train_targets, eval_targets\n",
    "\n",
    "    def create_sequences(self, data):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "\n",
    "        for i in range(len(data) - self.sequence_length - self.prediction_length):\n",
    "            sequences.append(data[i:i + self.sequence_length, :])\n",
    "            targets.append(data[i + self.sequence_length: i + self.sequence_length + self.prediction_length, 0])\n",
    "\n",
    "        return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb661f-8934-4d1f-941b-09643e5c690e",
   "metadata": {},
   "source": [
    "target_features = ['Global_active_power', 'Global_active_power', 'Voltage', 'Global_intensity', \n",
    "                   'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "24/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fa9b268-40e8-478e-bcb9-823a97c1e9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sequences shape:  (16896, 720, 13)\n",
      "train_targets shape:  (16896, 24)\n",
      "eval_sequences shape:  (4225, 720, 13)\n",
      "eval_targets shape:  (4225, 24)\n"
     ]
    }
   ],
   "source": [
    "# target_features = ['Global_active_power', 'Global_active_power', 'Voltage', 'Global_intensity', \n",
    "#                    'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "# file_path = 'data/hourly_household_power_consumption.csv'\n",
    "\n",
    "# target_features = ['Global_active_power', 'Global_intensity', \n",
    "#                    'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', \n",
    "#                    'Temp_Avg', 'Humidity_Avg']\n",
    "# file_path = 'data/final_hourly_power_weather_avg.csv'\n",
    "\n",
    "target_features = ['Global_active_power', 'Global_intensity', \n",
    "                   'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', \n",
    "                   'Temp_Avg', 'Humidity_Avg', 'sin_hour', 'cos_hour', \n",
    "                   'sin_day', 'cos_day', 'sin_month', 'cos_month']\n",
    "file_path = 'data/final_hourly_power_weather_avg_with_time.csv'\n",
    "\n",
    "\n",
    "sequence_length = 24*30\n",
    "prediction_length = 24\n",
    "\n",
    "dataset = PowerConsumptionDataset(file_path=file_path, \n",
    "                              sequence_length=sequence_length, prediction_length=prediction_length, \n",
    "                              target_features = target_features)\n",
    "\n",
    "train_sequences, eval_sequences, train_targets, eval_targets = dataset.load_data()\n",
    "\n",
    "print('train_sequences shape: ', train_sequences.shape)\n",
    "print('train_targets shape: ', train_targets.shape)\n",
    "print('eval_sequences shape: ', eval_sequences.shape)\n",
    "print('eval_targets shape: ', eval_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccd78af8-35d9-4ab7-8e82-0cd7cb5ccfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LSTM' or 'GRU' for LIME and SHAP, 'LSTM_Attention' for 'Attention' \n",
    "model_name = 'LSTM'  \n",
    "input_size = len(dataset.selected_features)  # Number of features\n",
    "hidden_size = 128\n",
    "num_layers = 2  \n",
    "output_size = prediction_length\n",
    "num_epochs = 100\n",
    "dropout = 0.3\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "num_of_features = input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3050da79-cb82-433b-bd5b-a33aadbcb17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/LSTM_ftnum_13_days_30_out_24.pth\n"
     ]
    }
   ],
   "source": [
    "model = create_model(model_name, input_size, \n",
    "                     hidden_size, num_layers, \n",
    "                     output_size, dropout)\n",
    "\n",
    "model_path = './tmp/{}_ftnum_{}_days_{}_out_{}.pth'.format(model_name, \n",
    "                                                           num_of_features,\n",
    "                                                           int(sequence_length/24),\n",
    "                                                           output_size)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07756873-fd5d-4e63-9b60-1eac98a0ee27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(13, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=24, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66687c13-f10f-4f40-96be-89c35e019f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate2(model, model_name, train_sequences, train_targets, \n",
    "                       eval_sequences, eval_targets, model_path, num_epochs=100, \n",
    "                       batch_size=64, learning_rate=0.001, patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Data loader\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_sequences, dtype=torch.float32), torch.tensor(train_targets, dtype=torch.float32))\n",
    "    eval_dataset = torch.utils.data.TensorDataset(torch.tensor(eval_sequences, dtype=torch.float32), torch.tensor(eval_targets, dtype=torch.float32))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = torch.utils.data.DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #, weight_decay=1e-5)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # tqdm을 사용하여 학습 진행도 표시\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for sequences_batch, targets_batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "                \n",
    "                # Move batch to GPU\n",
    "                sequences_batch, targets_batch = sequences_batch.to(device), targets_batch.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                if 'Attention' in model_name:\n",
    "                    outputs, _ = model(sequences_batch) \n",
    "                else:\n",
    "                    outputs = model(sequences_batch)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), targets_batch)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 현재 배치 손실을 누적\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Epoch 단위로 평균 손실 계산\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Evaluation on the validation set\n",
    "        model.eval()\n",
    "        eval_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for sequences_batch, targets_batch in eval_loader:\n",
    "                # Move batch to GPU\n",
    "                sequences_batch, targets_batch = sequences_batch.to(device), targets_batch.to(device)\n",
    "\n",
    "                if 'Attention' in model_name:\n",
    "                    val_outputs, _ = model(sequences_batch) \n",
    "                else:\n",
    "                    val_outputs = model(sequences_batch)\n",
    "                loss = criterion(val_outputs.squeeze(), targets_batch)\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        eval_loss /= len(eval_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {eval_loss:.4f}')\n",
    "\n",
    "        # Check if the validation loss improved\n",
    "        if eval_loss < best_val_loss:\n",
    "            best_val_loss = eval_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Validation loss improved. Model saved at epoch {epoch+1}\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping applied at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c668c26e-ff8f-4445-8cdf-b6c90aa3d85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pre-trained LSTM model...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Loading the pre-trained {model_name} model...\")\n",
    "    model = load_model(model, model_path)\n",
    "    # model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "else:\n",
    "    print(f\"Training a new {model_name} model...\")\n",
    "    train_and_evaluate2(model, model_name, train_sequences, train_targets, \n",
    "                       eval_sequences, eval_targets, model_path,\n",
    "                       num_epochs, batch_size, learning_rate, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59630ced-d300-4981-baa9-b374d57ce5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.9370\n",
      "0.9370015263557434\n"
     ]
    }
   ],
   "source": [
    "r2 = evaluate_r2_score(model, eval_sequences, eval_targets, model_name)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb069fc2-bf19-4f6c-9032-fa89c13d8f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5e92dbf-9592-4f35-98ea-f6dc65f10884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/LSTM_Attention_ftnum_13_days_30_out_24.pth\n",
      "LSTMWithAttention(\n",
      "  (lstm): LSTM(13, 256, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (attention): Attention(\n",
      "    (attention_layer): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=24, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hidden_size = 128, num_layers = 3 -> 0.5266\n",
    "hidden_size = 256, num_layers = 3 -> 0.9083 / 0.0869\n",
    "\n",
    "\"\"\"\n",
    "input_size = len(dataset.selected_features)  # Number of features\n",
    "hidden_size = 256\n",
    "num_layers = 3  \n",
    "output_size = prediction_length\n",
    "num_epochs = 100\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "num_of_features = input_size\n",
    "\n",
    "model_name = 'LSTM_Attention'  \n",
    "\n",
    "model = create_model(model_name, input_size, \n",
    "                     hidden_size, num_layers, \n",
    "                     output_size, dropout)\n",
    "\n",
    "model_path = './tmp/{}_ftnum_{}_days_{}_out_{}.pth'.format(model_name, \n",
    "                                                           num_of_features,\n",
    "                                                           int(sequence_length/24),\n",
    "                                                           output_size)\n",
    "print(model_path)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e94dd79-3e73-45f1-b83a-a20f02255182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pre-trained LSTM_Attention model...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Loading the pre-trained {model_name} model...\")\n",
    "    model = load_model(model, model_path)\n",
    "    # model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "else:\n",
    "    print(f\"Training a new {model_name} model...\")\n",
    "    train_and_evaluate2(model, model_name, train_sequences, train_targets, \n",
    "                       eval_sequences, eval_targets, model_path,\n",
    "                       num_epochs, batch_size, learning_rate, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6576f573-26aa-418f-a131-6b8d058e1d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.7514\n",
      "0.751369059085846\n"
     ]
    }
   ],
   "source": [
    "r2 = evaluate_r2_score(model, eval_sequences, eval_targets, model_name)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada2f2e-239d-4427-8818-ef9b6a0d5bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e18ca24-565b-407b-a6a5-a3d8aa48a48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77489816-c146-4b22-b14b-968db1a662e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTM'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f54176-0c24-45d1-ae4f-ce8f62250559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
